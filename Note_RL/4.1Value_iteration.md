why: find optimal policies

$v_{k+1}=\max _{\pi \in \Pi}\left(r_\pi+\gamma P_\pi v_k\right), \quad k=0,1,2, \ldots$

$\diamond$ The first step in every iteration is a policy update step. 

$$
\pi_{k+1}=\arg \max _\pi\left(r_\pi+\gamma P_\pi v_k\right),
$$

where $v_k$ is obtained in the previous iteration.

$\pi_{k+1}(s)=\arg \max _\pi \sum_a \pi(a \mid s) \underbrace{\left(\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_k\left(s^{\prime}\right)\right)}_{q_k(s, a)}, \quad s \in \mathcal{S}$.

$\pi_{k+1}(a \mid s)= \begin{cases}1, & a=a_k^*(s) \\ 0, & a \neq a_k^*(s)\end{cases}$

$a^*(s)=\arg \max _a q^*(a, s)$



$\diamond$ The second step is called a value update step. 

$$
v_{k+1}=r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}} v_k,
$$

where $v_{k+1}$ will be used in the next iteration.

$v_{k+1}(s)=\sum_a \pi_{k+1}(a \mid s) \underbrace{\left(\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_k\left(s^{\prime}\right)\right)}_{q_k(s, a)}, s \in \mathcal{S}$.

$v_{k+1}(s)=\max _a q_k(s, a)$.

In summary, the above steps can be illustrated as

$$
v_k(s) \rightarrow q_k(s, a) \rightarrow \text { new greedy policy } \pi_{k+1}(s) \rightarrow \text { new value } v_{k+1}(s)=\max _a q_k(s, a)
$$