Bellman equation

$\begin{aligned} v_\pi(s) & =\mathbb{E}\left[R_{t+1} \mid S_t=s\right]+\gamma \mathbb{E}\left[G_{t+1} \mid S_t=s\right], \\ & =\underbrace{\sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{r \in \mathcal{R}} p(r \mid s, a) r}_{\text {mean of immediate rewards }}+\underbrace{\gamma \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right)}_{\text {mean of future rewards }} \\ & =\sum_{a \in \mathcal{A}} \pi(a \mid s)\left[\sum_{r \in \mathcal{R}} p(r \mid s, a) r+\gamma \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right)\right], \quad \text { for all } s \in \mathcal{S} .\end{aligned}$

Bellman optimality equation (BOE)  

$\begin{aligned} v(s) & =\max _{\pi(s) \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a \mid s)\left(\sum_{r \in \mathcal{R}} p(r \mid s, a) r+\gamma \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) v\left(s^{\prime}\right)\right) \\ & =\max _{\pi(s) \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a \mid s) q(s, a)\end{aligned}$

**Solve**: There are two unknown variables $v(s)$ and $\pi(a \mid s)$, the first step is to solve $\pi$ on the right-hand side of the equation, because of $\sum_a \pi(a \mid s)=1$, regardless of $q(s,a)$ we have $\sum_{a \in \mathcal{A}} \pi(a \mid s) q(s, a) \leq \sum_{a \in \mathcal{A}} \pi(a \mid s) \max _{a \in \mathcal{A}} q(s, a)=\max _{a \in \mathcal{A}} q(s, a)$ , so $\pi^*$ is equal to
$$
\pi(a \mid s)= \begin{cases}1, & a=a^*, \\ 0, & a \neq a^*\end{cases}
$$

Here, $a^*=\arg \max _a q(s, a)$. In summary, the optimal policy $\pi(s)$ is the one that selects the action that has the greatest value of $q(s, a)$.

